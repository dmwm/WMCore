#!/usr/bin/env python
from __future__ import print_function, division
import json
import time
import httplib
import os

from pprint import pprint
from collections import defaultdict
from dbs.apis.dbsClient import DbsApi

endpoint = 'https://cmsweb.cern.ch:8443/dbs/prod/global/DBSReader'
dbs = DbsApi('https://cmsweb.cern.ch:8443/dbs/prod/global/DBSReader')

def callRESTAPI(restURL, url="cmsweb.cern.ch"):
    conn = httplib.HTTPSConnection(url, cert_file=os.getenv('X509_USER_CERT'),
                                 key_file=os.getenv('X509_USER_KEY'))
    conn.request('GET', restURL, headers={"Accept": "application/json"})
    r2 = conn.getresponse()
    data = r2.read()
    s = json.loads(data)
    return s


def listOfParentDatasets(dataset):
    parents = dbs.listDatasetParents(dataset=dataset)
    parentDatasets = []
    for parent in parents:
        parentDatasets.append(parent['parent_dataset'])
    return parentDatasets

def listBlocksWithNoParents(dataset):
    allBlocks = dbs.listBlocks(dataset=dataset)
    blockNames = []
    for block in allBlocks:
        blockNames.append(block['block_name'])
    parentBlocks = dbs.listBlockParents(block_name=blockNames)

    cblock = set()
    for pblock in parentBlocks:
        cblock.add(pblock['this_block_name'])

    noParentBlocks = set(blockNames) - cblock
    return noParentBlocks

def listFilesWithNoParents(blockName):
    allFiles = dbs.listFiles(block_name=blockName)
    parentFiles = dbs.listFileParents(block_name=blockName)

    allFileNames = set()
    for fInfo in allFiles:
        allFileNames.add(fInfo['logical_file_name'])

    cfile = set()
    for pFile in parentFiles:
        cfile.add(pFile['logical_file_name'])

    noParentFiles = allFileNames - cfile
    return noParentFiles

def getParentFilesFromDataset(parentDataset, lfn):
    fInfo = dbs.listFileLumis(logical_file_name=lfn)
    pLFNsByBlocks = defaultdict(set)
    for f in fInfo:
        pFiles = dbs.listFiles(dataset=parentDataset, run_num=f['run_num'], lumi_list=f['lumi_section_num'])
        for fi in pFiles:
            blocks = dbs.listBlocks(logical_file_name=fi['logical_file_name'])
            for bl in blocks:
                pLFNsByBlocks[bl['block_name']].add(fi['logical_file_name'])
    return pLFNsByBlocks

def getParentfilesMissingParents(dataset, searchFile=False):

    pDatasets = listOfParentDatasets(dataset)
    # print("parent datasets %s\n" % pDatasets)
    if not pDatasets:
        # print("No parents dataset found for %s\n" % dataset)
        return None, None

    if not searchFile:
        blocks = listBlocksWithNoParents(dataset)
    else:
        blocks = [b['block_name'] for b in dbs.listBlocks(dataset=dataset)]

    if blocks:
        # DatasetReport = {'child_dataset': dataset, 'parent_datasets': pDatasets, 'mssing_parent_blocks': []}
        DatasetReport = {'CDS': dataset, 'PDS': pDatasets, 'MISSING': []}
    else:
        return None, None

    print("=== searching problem dataset %s ===\n" % dataset)

    countChildFiles = 0
    countParentFiles = 0
    countChildBlocks = 0
    countParentBlocks = 0

    for blockName in blocks:
        # BlockLevel = {'child_block': blockName, "parent_blocks": set(), 'lfn_parentage': []}
        noParentFiles = listFilesWithNoParents(blockName)
        # print("%s block has %s files with no parents\n\n" % (blockName, len(noParentFiles)))
        if noParentFiles:
            BlockLevel = {'CBK': blockName, "PBK": [], 'PT': []}
            countChildBlocks += 1
        else:
            continue

        countChildFiles += len(noParentFiles)
        parentBlocks = set()
        for lfn in noParentFiles:
            # print("child file : %s\n" % lfn)
            parentlfns = []  # combined parent with different datasts
            for parentDataset in pDatasets:
                # print("found parents %s dataset" % parentDataset)
                plfns = getParentFilesFromDataset(parentDataset, lfn)
                # pprint("block, files %s" % dict(plfns))
                for pblock, pfiles in plfns.items():
                    # BlockLevel['parent_blocks'].append(pblock)
                    parentBlocks.add(pblock)
                    parentlfns.extend(list(pfiles))
                countParentFiles += len(parentlfns)
                # BlockLevel['lfn_parentage'].append({'child_lfn': lfn, 'parent_lfns': parentlfns})
            BlockLevel['PT'].append({'CLFN': lfn, 'PLFN': parentlfns})
        BlockLevel['PBK'] = list(parentBlocks)
        if searchFile:
            originalBlocks = dbs.listBlockParents(block_name=blockName)
            caculatedBlocks = set(BlockLevel['PBK'])
            if len(caculatedBlocks) != len(originalBlocks):
                print("Block doesn't match %s - %s" % (len(caculatedBlocks), len(originalBlocks)))
                print("Block: %s" % blockName)
                originalSet = set()
                for block in originalBlocks:
                    print(block)
                    originalSet.add(block['parent_block_name'])
                print("Difference from lumi-matching - original: %s" % (caculatedBlocks - originalSet))
                print("Difference original - lumi-matching: %s" % (originalSet - caculatedBlocks))

        countParentBlocks += len(BlockLevel['PBK'])
        # DatasetReport['missing_parent_blocks'].append(BlockLevel)
        DatasetReport['MISSING'].append(BlockLevel)
            # print()
    print("Total %s blocks with no parent child blocks (%s) found, %s parent bocks, %s child files, %s parent_files\n" % (
            len(blocks), countChildBlocks, countParentBlocks, countChildFiles, countParentFiles))
    metaData = {"NumChildDatasets": 1, "NumParentDatasets": len(pDatasets),
                "NumChildBlocks": countChildBlocks, "NumParentBlocks": countChildBlocks,
                "NumChildFiles": countChildFiles, "NumParentFiles": countParentFiles, }

    print("====================================\n\n")

    return DatasetReport, metaData


def getMissingParentSummaryUsingBlock(datasetList):
    totalMetaData = {"NumChildDatasets": 0, "NumParentDatasets": 0,
                "NumChildBlocks": 0, "NumParentBlocks": 0,
                "NumChildFiles": 0, "NumParentFiles": 0, }

    missingParents = []
    start = int(time.time())
    for dataset in datasetList:
        if isinstance(dataset, dict):
            dsName = dataset['dataset']
        else:
            dsName = dataset
        datasetInfo, metaData = getParentfilesMissingParents(dsName)

        if datasetInfo is not None:
            missingParents.append(datasetInfo)
            for k in totalMetaData:
                totalMetaData[k] += metaData[k]

    cEnd = int(time.time())

    with open("./missing_parents.json", "w") as f:
        json.dump(missingParents, f, indent=4)
    jEnd = int(time.time())

    with open("./missing_parents.py", "w") as f:
        f.write("MD = %s" % missingParents)

    pEnd = int(time.time())
    print("dbs call time %s" % (cEnd - start))
    print("json create time %s" % (jEnd - cEnd))
    print("python create time %s" % (pEnd - jEnd))
    print("total meta data:\n")
    pprint(totalMetaData)
    return missingParents


def getMissingParentSummaryUsingFile(dataset):
    missingParentsm, metaData = getParentfilesMissingParents(dataset, True)
    with open("./missing_parents_by_file.json", "w") as f:
        json.dump(missingParents, f, indent=4)
    with open("./missing_parents_by_file.py", "w") as f:
        f.write("MD = %s" % missingParents)
    return [missingParents]


def getOutputDSFromActiveWFs():
    restURL = '/wmstatsserver/data/filtered_requests?mask=OutputDatasets&mask=RequestType'
    result = callRESTAPI(restURL)["result"]

    outDataDict = {}
    for info in result:
        for outputDS in info["OutputDatasets"]:
            outDataDict.setdefault(outputDS, [])
            outDataDict[outputDS].append((info["RequestType"], info["RequestName"]))
    return outDataDict


def investigateACDCCollection(workflow):
    restURL = '/couchdb/acdcserver/_design/ACDC/_view/byCollectionName?key="%s"&reduce=false&include_docs=true' % workflow
    result = callRESTAPI(restURL)
    missingFiles = set()
    if "rows" in result:
        if result["rows"]:
            for row in result["rows"]:
                for inFile, value in row["doc"]["files"].items():
                    if not value["merged"] or value["merged"] == "0":
                        if value["parents"] and "/unmerged/" in value["parents"][0]:
                            missingFiles.add(inFile)
                        elif value["parents"]:
                            parentFiles = dbs.listFileArray(logical_file_name=value["parents"])
                            if len(parentFiles) != len(value["parents"]):
                                fromDBS = set([x["logical_file_name"] for x in preantFiles])
                                notInDBS = set(value["parents"]) - fromDBS
                                print(notInDBS)
                                exit(1)
        return missingFiles
    else:
        print(result)
        return missingFiles

if __name__ == '__main__':

    # datasetLists = dbs.listDatasets(min_cdate=1492717811, max_cdate=1498717900)
    # print(len(datasetLists))
    outDS = getOutputDSFromActiveWFs()
    datasetList = outDS.keys()
    missingPS = getMissingParentSummaryUsingBlock(datasetList)

    for info in missingPS:
        cDS = info['CDS']
        print("%s: %s" % (cDS, outDS[cDS]))

    wfs = set()
    for ds, value in outDS.items():
        for reqInfo in value:
            if reqInfo[0] != "Resubmission":
                wfs.add(reqInfo[1])
    missingFiles = set()
    for wf in wfs:
        missingFiles = missingFiles.union(investigateACDCCollection(wf))

    with open("./child_with_umerged_parents.json", "w") as f:
        json.dump(list(missingFiles), f, indent=4)

    print(len(missingFiles))

    # getMissingParentSummaryUsingFile("/SingleMuon/Run2016D-03Feb2017-v1/MINIAOD")
